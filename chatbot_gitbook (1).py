# -*- coding: utf-8 -*-
"""ChatBot_GitBook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jA_r0Ns_nrmeVMeZNJKV7TsHhdLCt1NW

# Understanding Problem and Objective
"""

# The objective of this project is to create a Chatbot which can query topics mentioned in GitBook Documentation.
# The project will help summarize topics without going through all pages of the GitBook documentation.
# The project is executed using RAG approach:
  #Step1: Loadind Data
  #Step2: Chunking/Split
  #Step3: Tokenizing & Embedding Chunks
  #Step4: Indexing & Storing Context
  #Step5: Retrieving & Generate Response

"""# Step1: Loading Data"""

! pip install langchain_community

# import required libraries
import langchain
from langchain_community.document_loaders import WebBaseLoader
import bs4

#load all details from gitbook documentation
loader = WebBaseLoader(web_paths=("https://docs.gitbook.com/",))
# all_docs = loader.load_and_split()
all_docs = loader.load()
all_docs

all_docs[0].page_content

"""#Step2: Chunking of Data"""

print("No of characters in page content : ", len(all_docs[0].page_content))

!pip install langchain-text-splitters

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 100,
    chunk_overlap = 10
)
doc_splits = text_splitter.split_documents(all_docs)
doc_splits

"""# Step3: Tokenizing & Embedding Chunks"""

!pip install transformers

from transformers import AutoTokenizer
model_name_or_path = "TheBloke/Llama-2-7b-Chat-GPTQ"
model_basename = "gptq_model-4bit-128g"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)

tokens_count = [len(tokenizer(doc.page_content, return_tensors='pt').input_ids.cuda()[0]) for doc in doc_splits]
tokens_count

def token_len(text):
  return len(tokenizer(text, return_tensors='pt').input_ids.cuda()[0])

!pip install langchain-huggingface

from langchain_huggingface import HuggingFaceEmbeddings

model_embed = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2") #This model tokenizes and embeds the text. Hence, no need of executing AutoTokenizer on Ducument Chunks.

embeddings = model_embed.embed_documents((doc.page_content for doc in doc_splits))
len(embeddings[0]) # tokenized and embedded texts

"""# Step4: Indexing & Storing"""

!pip install langchain-pinecone

from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec

pc_store = Pinecone(api_key="pcsk_2htMdG_61tE3AyPDo585E3Z5W3TdjaTRdG7xMSY7j6fpeNZkgJMSBFU7jy3rMKB9Y8NbJR")

index_name = "gitbook"

if pc_store.has_index(index_name):
  print("Index already exists")
  # pc_store.delete_index(index_name)
  index = pc_store.Index(index_name)
else:
  pc_store.create_index(
      name = index_name,
      dimension = len(embeddings[0]),
      metric = 'cosine',
      spec = ServerlessSpec(
          cloud='aws', region='us-east-1')
  )
  index = pc_store.Index(index_name)

print(index.describe_index_stats())

#Create a vector store which will embed using the embedding model and store into vector db at one go
vector_store = PineconeVectorStore(
    index=  index,
    embedding = model_embed
)

#Store embeddings in vector DB
doc_ids = vector_store.add_documents(doc_splits)
doc_ids

#check if storage is done correctly
print(index.describe_index_stats())

"""#Step5: Retrieving/Querying Data"""

!pip install langgraph

!pip install "langchain[groq]"

#define the LLM model
from langchain_groq import ChatGroq
import os
import getpass
if not os.environ.get("GROQ_API_KEY"):
  os.environ["GROQ_API_KEY"] = getpass.getpass("Enter API Key for Groq: ")

llm_model = ChatGroq(model = "llama-3.1-8b-instant", groq_api_key= os.environ["GROQ_API_KEY"])

# from langchain.chat_models import init_chat_model
# llm_model = init_chat_model("llama-3.1-8b-instant", model_provider="groq")

#import required libaries for retrieval and response generation
from langchain import hub #used for system prompts
from langgraph.graph import StateGraph, START
from langchain_core.documents import Document
from typing_extensions import List, TypedDict

#define system prompts
prompt = hub.pull("rlm/rag-prompt")

#define State to store context, query and response
class State(TypedDict):
  question : str
  answer : str
  context : List[Document]

#define retrieval steps using similarity search
def retrieve(state: State):
  retrieved_docs = vector_store.similarity_search(state['question']) #this will implicitly tokenize and embed query
  state['context'] = retrieved_docs
  return {"context": retrieved_docs}

#define querying mechanism
def generate_response(state: State):
  docs_content = "\n\n".join(doc.page_content for doc in state['context'])
  messages = prompt.invoke({"question": state['question'], "context": docs_content})
  response = llm_model.invoke(messages)
  state['answer'] = response.content
  return {"answer": response.content}

#compile the chatbot
graph_builder = StateGraph(State).add_sequence([retrieve, generate_response]) #initialize the graph
graph_builder.add_edge(START, "retrieve") #set the starting node in the graph
graph = graph_builder.compile()

# Use the chatbot by sending your query
reponse = graph.invoke({"question": "What is GitBook?"})
print(reponse['answer'])

